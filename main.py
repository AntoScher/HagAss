import json
import torch
from torch.utils.data import Dataset, Subset
from transformers import (
    AutoTokenizer,
    AutoModelForCausalLM,
    TrainingArguments,
    Trainer,
    DataCollatorForLanguageModeling
)

# 1. –ó–∞–≥—Ä—É–∑–∫–∞ —Å–∏—Å—Ç–µ–º–Ω–æ–≥–æ –ø—Ä–æ–º–ø—Ç–∞
with open('system_prompt.txt', 'r', encoding='utf-8') as f:
    SYSTEM_PROMPT = f.read().strip()

# 2. –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–æ–Ω–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
MAX_INPUT_LENGTH = 384  # –£–º–µ–Ω—å—à–µ–Ω–æ –¥–ª—è —Ä–µ–∑–µ—Ä–≤–∞ –ø–æ–¥ –≥–µ–Ω–µ—Ä–∞—Ü–∏—é
MAX_NEW_TOKENS = 128  # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ –æ—Ç–≤–µ—Ç–∞


# 3. –ö–ª–∞—Å—Å –¥–∞—Ç–∞—Å–µ—Ç–∞ —Å —É—á–µ—Ç–æ–º –ª–∏–º–∏—Ç–æ–≤
class PharmacyDataset(Dataset):
    def __init__(self, file_path, tokenizer):
        self.data = []
        self.tokenizer = tokenizer

        with open(file_path, 'r', encoding='utf-8') as f:
            raw_data = json.load(f)

            for item in raw_data:
                text = f"{SYSTEM_PROMPT}\n\n–í–æ–ø—Ä–æ—Å: {item['–í–æ–ø—Ä–æ—Å']}\n–û—Ç–≤–µ—Ç: {item['–û—Ç–≤–µ—Ç']}"
                self.data.append(text)

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        encoding = self.tokenizer(
            self.data[idx],
            max_length=MAX_INPUT_LENGTH,
            padding='max_length',
            truncation=True,
            return_tensors='pt'
        )
        return {k: v.squeeze() for k, v in encoding.items()}


# 4. –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ–¥–µ–ª–∏
model_name = "microsoft/DialoGPT-medium"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(model_name)
tokenizer.pad_token = tokenizer.eos_token
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)

# 5. –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö
dataset = PharmacyDataset(
    file_path="D:/DEV/HagAss/pharmacy_data.json",
    tokenizer=tokenizer
)
train_size = int(0.9 * len(dataset))
train_dataset = Subset(dataset, range(train_size))
val_dataset = Subset(dataset, range(train_size, len(dataset)))

# 6. –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –æ–±—É—á–µ–Ω–∏—è
training_args = TrainingArguments(
    output_dir="./results",
    per_device_train_batch_size=2,
    per_device_eval_batch_size=2,
    num_train_epochs=7,
    learning_rate=3e-5,
    logging_dir="./logs",
    save_steps=500,
    eval_strategy="steps",
    eval_steps=500,
    logging_steps=100,
    fp16=torch.cuda.is_available(),
    gradient_accumulation_steps=2,
)

data_collator = DataCollatorForLanguageModeling(
    tokenizer=tokenizer,
    mlm=False
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=val_dataset,
    data_collator=data_collator,
)

# 7. –û–±—É—á–µ–Ω–∏–µ
trainer.train()


# 8. –ò—Å–ø—Ä–∞–≤–ª–µ–Ω–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–æ–≤
def generate_response(user_input):
    # –§–æ—Ä–º–∏—Ä—É–µ–º –ø–æ–ª–Ω—ã–π –ø—Ä–æ–º–ø—Ç
    full_prompt = f"{SYSTEM_PROMPT}\n\n–í–æ–ø—Ä–æ—Å: {user_input}\n–û—Ç–≤–µ—Ç:"

    # –ö–æ–¥–∏—Ä—É–µ–º —Å —É—á–µ—Ç–æ–º –ª–∏–º–∏—Ç–æ–≤
    inputs = tokenizer.encode_plus(
        full_prompt,
        return_tensors="pt",
        max_length=MAX_INPUT_LENGTH,
        padding='max_length',
        truncation=True
    ).to(device)

    # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —Å –∫–æ–Ω—Ç—Ä–æ–ª–µ–º –¥–ª–∏–Ω—ã
    outputs = model.generate(
        input_ids=inputs['input_ids'],
        attention_mask=inputs['attention_mask'],
        max_new_tokens=MAX_NEW_TOKENS,  # –ö–ª—é—á–µ–≤–æ–µ –∏–∑–º–µ–Ω–µ–Ω–∏–µ!
        pad_token_id=tokenizer.eos_token_id,
        temperature=0.85,
        top_k=100,
        top_p=0.95,
        repetition_penalty=1.15
    )

    # –í—ã—Ä–µ–∑–∞–µ–º —Ç–æ–ª—å–∫–æ –æ—Ç–≤–µ—Ç
    return tokenizer.decode(
        outputs[0][inputs['input_ids'].shape[-1]:],
        skip_special_tokens=True
    )


# 9. –û–±—Ä–∞–±–æ—Ç–∫–∞ –≤–≤–æ–¥–∞
def process_input(text):
    emergency_keywords = ["—Å–∫–æ—Ä–∞—è", "—É–º–∏—Ä–∞—é", "–∫—Ä–æ–≤—å", "–ø–æ—Ç–µ—Ä—è–ª —Å–æ–∑–Ω–∞–Ω–∏–µ"]
    if any(kw in text.lower() for kw in emergency_keywords):
        return "‚ö†Ô∏è –°—Ä–æ—á–Ω–æ –≤—ã–∑–æ–≤–∏—Ç–µ —Å–∫–æ—Ä—É—é (103)! –ü–æ—Å–ª–µ –ø–æ–º–æ—â–∏ –ø–æ–º–æ–∂–µ–º —Å –ª–µ–∫–∞—Ä—Å—Ç–≤–∞–º–∏."

    if any(kw in text.lower() for kw in ["–¥–æ—Å—Ç–∞–≤–∫–∞", "–≥–æ—Ä–æ–¥", "–∑–∞–∫–∞–∑–∞—Ç—å"]):
        return "üè™ –î–ª—è –∑–∞–∫–∞–∑–∞ —É–∫–∞–∂–∏—Ç–µ:\n1. –ì–æ—Ä–æ–¥\n2. –ü—Ä–µ–ø–∞—Ä–∞—Ç\n3. –°–ø–æ—Å–æ–± –ø–æ–ª—É—á–µ–Ω–∏—è"

    response = generate_response(text)
    return response if len(response) > 2 else "–£—Ç–æ—á–Ω–∏—Ç–µ –∑–∞–ø—Ä–æ—Å, –ø–æ–∂–∞–ª—É–π—Å—Ç–∞."


# 10. –ó–∞–ø—É—Å–∫ —Å–∏—Å—Ç–µ–º—ã
if __name__ == "__main__":
    model.save_pretrained("./pharmacy_gpt")
    tokenizer.save_pretrained("./pharmacy_gpt")

    print("–°–∏—Å—Ç–µ–º–∞ –≥–æ—Ç–æ–≤–∞. –î–ª—è –≤—ã—Ö–æ–¥–∞ –≤–≤–µ–¥–∏—Ç–µ '–≤—ã—Ö–æ–¥'")
    while True:
        user_input = input("–í—ã: ")
        if user_input.lower() in ["–≤—ã—Ö–æ–¥", "exit"]:
            break
        print("–ê–ø—Ç–µ–∫–∞—Ä—å:", process_input(user_input))