import json
import torch
from torch.utils.data import Dataset, Subset
from transformers import (
    AutoTokenizer,
    AutoModelForCausalLM,
    TrainingArguments,
    Trainer,
    DataCollatorForLanguageModeling
)


# 2. –ó–∞–≥—Ä—É–∑–∫–∞ –∏ –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö
class PharmacyDataset(Dataset):
    def __init__(self, file_path, tokenizer, max_length=512):
        self.data = []
        self.tokenizer = tokenizer
        self.max_length = max_length

        with open(file_path, 'r', encoding='utf-8') as f:
            raw_data = json.load(f)
            for item in raw_data:
                text = f"–í–æ–ø—Ä–æ—Å: {item['–í–æ–ø—Ä–æ—Å']}\n–û—Ç–≤–µ—Ç: {item['–û—Ç–≤–µ—Ç']}"
                self.data.append(text)

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        text = self.data[idx]
        encoding = self.tokenizer(
            text,
            max_length=self.max_length,
            padding='max_length',
            truncation=True,
            return_tensors='pt'
        )
        return {key: val.squeeze() for key, val in encoding.items()}


# 3. –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ–¥–µ–ª–∏ –∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞
model_name = "microsoft/DialoGPT-medium"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(model_name)
tokenizer.pad_token = tokenizer.eos_token

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞ –ø–æ—Å–ª–µ —Å–æ–∑–¥–∞–Ω–∏—è –º–æ–¥–µ–ª–∏
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)

# 4. –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞
dataset = PharmacyDataset(
    file_path="D:/DEV/HagAss/pharmacy_data.json",
    tokenizer=tokenizer
)
data_collator = DataCollatorForLanguageModeling(
    tokenizer=tokenizer,
    mlm=False  # –î–ª—è CausalLM –∑–∞–¥–∞—á–∏
)
# 5. –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö
train_size = int(0.9 * len(dataset))
train_dataset = Subset(dataset, range(train_size))
val_dataset = Subset(dataset, range(train_size, len(dataset)))

# 6. –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –æ–±—É—á–µ–Ω–∏—è
training_args = TrainingArguments(
    output_dir="./results",
    per_device_train_batch_size=2,
    per_device_eval_batch_size=2,
    num_train_epochs=7,
    logging_dir="./logs",
    save_steps=500,
    eval_strategy="steps",  # –ë—ã–ª–æ evaluation_strategy
    eval_steps=500,
    logging_steps=100,
)

data_collator = DataCollatorForLanguageModeling(
    tokenizer=tokenizer,
    mlm=False
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=val_dataset,
    data_collator=data_collator,
)

# 7. –ó–∞–ø—É—Å–∫ –æ–±—É—á–µ–Ω–∏—è
trainer.train()


# 8. –§—É–Ω–∫—Ü–∏–∏ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç–≤–µ—Ç–æ–≤
def generate_response(user_input):
    inputs = tokenizer.encode(
        user_input + tokenizer.eos_token,
        return_tensors="pt",
        padding='max_length',  # –î–æ–±–∞–≤–ª–µ–Ω–æ
        max_length=512,  # –î–æ–±–∞–≤–ª–µ–Ω–æ
        truncation=True  # –î–æ–±–∞–≤–ª–µ–Ω–æ
    ).to(device)  # –î–æ–±–∞–≤–ª–µ–Ω–æ –ø–µ—Ä–µ–º–µ—â–µ–Ω–∏–µ –Ω–∞ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ

    outputs = model.generate(
        input_ids=inputs['input_ids'],
        attention_mask=inputs['attention_mask'],  # –ö–ª—é—á–µ–≤–æ–µ –∏–∑–º–µ–Ω–µ–Ω–∏–µ
        max_length=500,
        pad_token_id=tokenizer.eos_token_id,
        no_repeat_ngram_size=3,
        do_sample=True,
        top_k=82,
        top_p=0.85,
        temperature=0.7,
        repetition_penalty=1.2  # –®—Ç—Ä–∞—Ñ –∑–∞ –ø–æ–≤—Ç–æ—Ä–µ–Ω–∏—è
    )

    #return tokenizer.decode(outputs[:, inputs.shape[-1]:][0], skip_special_tokens=True)
    return tokenizer.decode(outputs[:, inputs['input_ids'].shape[-1]:][0], skip_special_tokens=True)

def process_input(text):
    emergency_keywords = ["—Å–∫–æ—Ä–∞—è", "—É–º–∏—Ä–∞—é", "–∫—Ä–æ–≤—å", "–ø–æ—Ç–µ—Ä—è–ª —Å–æ–∑–Ω–∞–Ω–∏–µ"]
    if any(kw in text.lower() for kw in emergency_keywords):
        return "‚ö†Ô∏è –°—Ä–æ—á–Ω–æ –≤—ã–∑–æ–≤–∏—Ç–µ —Å–∫–æ—Ä—É—é –ø–æ–º–æ—â—å (103)! –ü–æ—Å–ª–µ –æ–∫–∞–∑–∞–Ω–∏—è –ø–æ–º–æ—â–∏ –º—ã –ø–æ–º–æ–∂–µ–º —Å –ª–µ–∫–∞—Ä—Å—Ç–≤–∞–º–∏."

    if any(kw in text.lower() for kw in ["–¥–æ—Å—Ç–∞–≤–∫–∞", "–≥–æ—Ä–æ–¥", "–∑–∞–∫–∞–∑–∞—Ç—å"]):
        return phase_1_sales(text)

    return generate_response(text)


def phase_1_sales(text):
    return "üè™ –î–ª—è –æ—Ñ–æ—Ä–º–ª–µ–Ω–∏—è –∑–∞–∫–∞–∑–∞ —É–∫–∞–∂–∏—Ç–µ:\n1. –ì–æ—Ä–æ–¥\n2. –ù–∞–∑–≤–∞–Ω–∏–µ –ø—Ä–µ–ø–∞—Ä–∞—Ç–∞\n3. –°–ø–æ—Å–æ–± –ø–æ–ª—É—á–µ–Ω–∏—è (–¥–æ—Å—Ç–∞–≤–∫–∞/—Å–∞–º–æ–≤—ã–≤–æ–∑)"


# 9. –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
if __name__ == "__main__":
    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ –ø–æ—Å–ª–µ –æ–±—É—á–µ–Ω–∏—è
    model.save_pretrained("./pharmacy_gpt")
    tokenizer.save_pretrained("./pharmacy_gpt")

    # –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ
    while True:
        user_input = input("–í—ã: ")
        if user_input.lower() in ["exit", "–≤—ã—Ö–æ–¥"]:
            break
        print("–ê–ø—Ç–µ–∫–∞—Ä—å:", process_input(user_input))